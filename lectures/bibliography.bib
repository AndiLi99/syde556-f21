@misc{richard1988richard,
  title = {Richard {{Feynman}}'s Blackboard at Time of His Death},
  url = {http://archives-dc.library.caltech.edu/islandora/object/ct1:483},
  publisher = {{The Caltech Archives}},
  urldate = {2019-12-30},
  date = {1988},
  author = {Feynman, Richard}
}

@book{eliasmith2013how,
  location = {{New York, New York}},
  title = {How to Build a Brain: {{A}} Neural Architecture for Biological Cognition},
  isbn = {978-0-19-026212-9},
  pagetotal = {456},
  series = {Oxford {{Series}} on {{Cognitive Models}} and {{Architectures}}},
  publisher = {{Oxford University Press}},
  date = {2013},
  author = {Eliasmith, Chris},
  organization = {Oxford University Press}
}

@book{eliasmith2003neural,
  location = {{Cambridge, Massachusetts}},
  title = {Neural Engineering: {{Computation}}, Representation, and Dynamics in Neurobiological Systems},
  isbn = {978-0-262-55060-4},
  pagetotal = {380},
  publisher = {{MIT Press}},
  date = {2003},
  author = {Eliasmith, Chris and Anderson, Charles H.}
}

@article{bekolay2014nengo,
  title = {Nengo: {{A Python}} Tool for Building Large-Scale Functional Brain Models},
  volume = {7},
  abstract = {Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results.},
  number = {48},
  journaltitle = {Frontiers in Neuroinformatics},
  date = {2014},
  author = {Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron R. and Eliasmith, Chris}
}

@article{hafting2005microstructure,
  title = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  volume = {436},
  issn = {1476-4687},
  url = {https://doi.org/10.1038/nature03721},
  doi = {10.1038/nature03721},
  abstract = {The ability to find one's way depends on neural algorithms that integrate information about place, distance and direction, but the implementation of these operations in cortical microcircuits is poorly understood. Here we show that the dorsocaudal medial entorhinal cortex (dMEC) contains a directionally oriented, topographically organized neural map of the spatial environment. Its key unit is the ‘grid cell’, which is activated whenever the animal's position coincides with any vertex of a regular grid of equilateral triangles spanning the surface of the environment. Grids of neighbouring cells share a common orientation and spacing, but their vertex locations (their phases) differ. The spacing and size of individual fields increase from dorsal to ventral dMEC. The map is anchored to external landmarks, but persists in their absence, suggesting that grid cells may be part of a generalized, path-integration-based map of the spatial environment.},
  number = {7052},
  journaltitle = {Nature},
  shortjournal = {Nature},
  date = {2005-08-01},
  pages = {801-806},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.},
}

@book{okeefe1978hippocampus,
  location = {{Oxford, United Kingdom}},
  title = {The {{Hippocampus}} as a {{Cognitive Map}}},
  isbn = {0-19-857206-9},
  url = {http://cognitivemap.net/},
  publisher = {{Oxford University Press}},
  date = {1978},
  author = {O'Keefe, John and Nadel, Lynn},
}

@article{sussillo2009generating,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  volume = {63},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627309005479},
  doi = {https://doi.org/10.1016/j.neuron.2009.07.018},
  abstract = {Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  number = {4},
  journaltitle = {Neuron},
  date = {2009},
  pages = {544-557},
  keywords = {SYSNEURO},
  author = {Sussillo, David and Abbott, Laurence F.},
}


@article{nicola2017supervised,
  title = {Supervised Learning in Spiking Neural Networks with {{FORCE}} Training},
  volume = {8},
  issn = {2041-1723},
  url = {https://doi.org/10.1038/s41467-017-01827-3},
  doi = {10.1038/s41467-017-01827-3},
  abstract = {Populations of neurons display an extraordinary diversity in the behaviors they affect and display. Machine learning techniques have recently emerged that allow us to create networks of model neurons that display behaviors of similar complexity. Here we demonstrate the direct applicability of one such technique, the FORCE method, to spiking neural networks. We train these networks to mimic dynamical systems, classify inputs, and store discrete sequences that correspond to the notes of a song. Finally, we use FORCE training to create two biologically motivated model circuits. One is inspired by the zebra finch and successfully reproduces songbird singing. The second network is motivated by the hippocampus and is trained to store and replay a movie scene. FORCE trained networks reproduce behaviors comparable in complexity to their inspired circuits and yield information not easily obtainable with other techniques, such as behavioral responses to pharmacological manipulations and spike timing statistics.},
  number = {1},
  journaltitle = {Nature Communications},
  shortjournal = {Nature Communications},
  date = {2017-12-20},
  pages = {2208},
  author = {Nicola, Wilten and Clopath, Claudia},
}

@article{boerlin2011spikebased,
  title = {Spike-{{Based Population Coding}} and {{Working Memory}}},
  volume = {7},
  url = {https://doi.org/10.1371/journal.pcbi.1001080},
  doi = {10.1371/journal.pcbi.1001080},
  abstract = {Author Summary Most of our daily actions are subject to uncertainty. Behavioral studies have confirmed that humans handle this uncertainty in a statistically optimal manner. A key question then is what neural mechanisms underlie this optimality, i.e. how can neurons represent and compute with probability distributions. Previous approaches have proposed that probabilities are encoded in the firing rates of neural populations. However, such rate codes appear poorly suited to understand perception in a constantly changing environment. In particular, it is unclear how probabilistic computations could be implemented by biologically plausible spiking neurons. Here, we propose a network of spiking neurons that can optimally combine uncertain information from different sensory modalities and keep this information available for a long time. This implies that neural memories not only represent the most likely value of a stimulus but rather a whole probability distribution over it. Furthermore, our model suggests that each spike conveys new, essential information. Consequently, the observed variability of neural responses cannot simply be understood as noise but rather as a necessary consequence of optimal sensory integration. Our results therefore question strongly held beliefs about the nature of neural “signal” and “noise”.},
  number = {2},
  journaltitle = {PLOS Computational Biology},
  date = {2011-02},
  pages = {1-18},
  author = {Boerlin, Martin and Denève, Sophie},
  publisher = {Public Library of Science}
}

@article{boerlin2013predictive,
  title = {Predictive {{Coding}} of {{Dynamical Variables}} in {{Balanced Spiking Networks}}},
  volume = {9},
  url = {https://doi.org/10.1371/journal.pcbi.1003258},
  doi = {10.1371/journal.pcbi.1003258},
  abstract = {Author Summary Two observations about the cortex have puzzled and fascinated neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks representing information reliably and with a small number of spikes. To achieve such efficiency, spikes of individual neurons must communicate prediction errors about a common population-level signal, automatically resulting in balanced excitation and inhibition and highly variable neural responses. We illustrate our approach by focusing on the implementation of linear dynamical systems. Among other things, this allows us to construct a network of spiking neurons that can integrate input signals, yet is robust against many perturbations. Most importantly, our approach shows that neural variability cannot be equated to noise. Despite exhibiting the same single unit properties as other widely used network models, our balanced networks are orders of magnitudes more reliable. Our results suggest that the precision of cortical representations has been strongly underestimated.},
  number = {11},
  journaltitle = {PLOS Computational Biology},
  date = {2013-11},
  pages = {1-16},
  author = {Boerlin, Martin and Machens, Christian K. and Denève, Sophie},
  publisher = {Public Library of Science}
}


@article{eliasmith2005unified,
  title = {A Unified Approach to Building and Controlling Spiking Attractor Networks},
  volume = {7},
  number = {6},
  journaltitle = {Neural computation},
  date = {2005},
  pages = {1276-1314},
  author = {Eliasmith, Chris},
  type = {article}
}

@book{abbott2001theoretical,
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  isbn = {978-0-262-04199-7},
  url = {https://mitpress.mit.edu/books/theoretical-neuroscience},
  abstract = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.

The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  pagetotal = {480},
  series = {Computational {{Neuroscience}}},
  publisher = {{MIT Press}},
  date = {2001-10},
  author = {Abbott, Laurence F. and Dayan, Peter},
}

@mastersthesis{stoeckel2015design,
  location = {{Germany}},
  title = {Design Space Exploration of Associative Memories Using Spiking Neurons with Respect to Neuromorphic Hardware Implementations},
  institution = {{Bielefeld University}},
  date = {2015},
  author = {Stöckel, Andreas}
}

@book{kandel2012principles,
  title = {Principles of {{Neural Science}}},
  edition = {5},
  publisher = {{McGraw-Hill Education}},
  date = {2012},
  author = {Kandel, E. and Schwartz, J. and Jessell, T. and Siegelbaum, S. and Hudspeth, A. J.}
}

@article{chedotal2010wiring,
  langid = {english},
  title = {Wiring the Brain: The Biology of Neuronal Guidance},
  volume = {2},
  issn = {1943-0264},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/20463002},
  doi = {10.1101/cshperspect.a001917},
  abstract = {The mammalian brain is the most complex organ in the body. It controls all aspects of our bodily functions and interprets the world around us through our senses. It defines us as human beings through our memories and our ability to plan for the future. Crucial to all these functions is how the brain is wired in order to perform these tasks. The basic map of brain wiring occurs during embryonic and postnatal development through a series of precisely orchestrated developmental events regulated by specific molecular mechanisms. Below we review the most important features of mammalian brain wiring derived from work in both mammals and in nonmammalian species. These mechanisms are highly conserved throughout evolution, simply becoming more complex in the mammalian brain. This fascinating area of biology is uncovering the essence of what makes the mammalian brain able to perform the everyday tasks we take for granted, as well as those which give us the ability for extraordinary achievement.},
  number = {6},
  journaltitle = {Cold Spring Harbor perspectives in biology},
  shortjournal = {Cold Spring Harb Perspect Biol},
  date = {2010-06},
  pages = {a001917-a001917},
  keywords = {Humans,Animals,Axons/physiology,Brain/*cytology/*physiology,Gene Expression Regulation/physiology,Mammals/*physiology,Neurons/*cytology/*physiology},
  author = {Chédotal, Alain and Richards, Linda J},
  eprinttype = {pubmed},
  eprint = {20463002}
}

@article{kanwisher1997fusiform,
  title = {The Fusiform Face Area: {{A}} Module in Human Extrastriate Cortex Specialized for Face Perception},
  volume = {17},
  issn = {0270-6474},
  url = {https://www.jneurosci.org/content/17/11/4302},
  doi = {10.1523/JNEUROSCI.17-11-04302.1997},
  abstract = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate face area also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area FF) that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is selectively involved in the perception of faces.},
  number = {11},
  journaltitle = {Journal of Neuroscience},
  date = {1997},
  pages = {4302-4311},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  eprint = {https://www.jneurosci.org/content/17/11/4302.full.pdf},
  publisher = {{Society for Neuroscience}}
}

@article{markram2012human,
  title = {The {{Human Brain Project}}},
  volume = {306},
  number = {6},
  journaltitle = {Scientific American},
  date = {2012},
  pages = {50-55},
  author = {Markram, Henry}
}

@article{merolla2014million,
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  volume = {345},
  issn = {0036-8075},
  url = {https://science.sciencemag.org/content/345/6197/668},
  doi = {10.1126/science.1254642},
  abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brains structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  number = {6197},
  journaltitle = {Science},
  date = {2014},
  pages = {668-673},
  author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  eprint = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  publisher = {{American Association for the Advancement of Science}}
}

@article{morgan2011darpa,
  entrysubtype = {newspaper},
  title = {{{DARPA}} Shells out \$21m for {{IBM}} Cat Brain Chip},
  url = {https://www.theregister.co.uk/2011/08/18/ibm_darpa_synapse_project/},
  journaltitle = {The Register},
  journalsubtitle = {Science},
  urldate = {2020-01-03},
  date = {2011-08-18},
  author = {Morgan, Timothy P.}
}

@article{komer2016unified,
  title = {A Unified Theoretical Approach for Biological Cognition and Learning},
  volume = {11},
  issn = {2352-1546},
  url = {http://www.sciencedirect.com/science/article/pii/S2352154616300651},
  doi = {https://doi.org/10.1016/j.cobeha.2016.03.006},
  abstract = {Large-scale neural models are needed in order to understand the biological underpinnings of complex cognitive behavior. Good methods for constructing such models should provide for: first, abstraction (analysis across levels of description); second, integration (incorporation of simpler models to build more complex ones); third, empirical contact (using and comparing to a wide variety of neural data); and fourth, account for the varieties of learning. In this review we evaluate three prominent recent methods for constructing neural models using these four criteria. Each of these methods is being actively developed and demonstrates clear strengths along some of these criteria.},
  journaltitle = {Current Opinion in Behavioral Sciences},
  date = {2016},
  pages = {14-20},
  author = {Komer, Brent and Eliasmith, Chris},
  note = {Computational modeling}
}

@article{jonas2017could,
  title = {Could a Neuroscientist Understand a Microprocessor?},
  volume = {13},
  url = {https://doi.org/10.1371/journal.pcbi.1005268},
  doi = {10.1371/journal.pcbi.1005268},
  abstract = {Author Summary Neuroscience is held back by the fact that it is hard to evaluate if a conclusion is correct; the complexity of the systems under study and their experimental inaccessability make the assessment of algorithmic and data analytic technqiues challenging at best. We thus argue for testing approaches using known artifacts, where the correct interpretation is known. Here we present a microprocessor platform as one such test case. We find that many approaches in neuroscience, when used naïvely, fall short of producing a meaningful understanding.},
  number = {1},
  journaltitle = {PLOS Computational Biology},
  date = {2017-01},
  pages = {1-24},
  author = {Jonas, Eric and Kording, Konrad Paul},
  publisher = {{Public Library of Science}}
}

@article{boahen2017neuromorph,
  title = {A Neuromorph's Prospectus},
  volume = {19},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2017.33},
  number = {2},
  journaltitle = {Computing in Science Engineering},
  date = {2017-03},
  pages = {14-28},
  keywords = {neuromorphic,computational neuroscience,neural engineering,Neuromorphics,Neural networks,Neuroscience,all-analog computing,all-digital computing,analog dendritic computation,analogue integrated circuits,asynchronous circuits,asynchronous digital circuits,asynchronous logic,autonomous robots,axons,cognitive computing,continuous signal encoding,dendrite emulation,digital axonal communication,Digital communication,digital computers,digital integrated circuits,electron traffic,embedded computing,Energy efficiency,error tolerance,ion channels,mapping arbitrary computations,mixed analog-digital systems,Moore's Law,Nanoscale devices,nanoscale dimensions,network-on-a-chip,neural networks,neurogrid neuromorphic system,neuromorph prospectus,neuromorphic chips,neuromorphic engineers,scientific computing,shrink transistors,single-lane nanoscale devices,spike trains,subthreshold analog circuits,subthreshold circuits,synaptic connections,system-on-a-chip,Three-dimensional displays,Transistors,trapped electrons--blocking lanes,Very large scale integration,VLSI},
  author = {Boahen, Kwabena}
}

@book{splittgerber2018snell,
  title = {Snell's {{Clinical Neuroanatomy}}},
  edition = {8th Edition},
  abstract = {Snell’s Clinical Neuroanatomy , Eighth Edition, equips medical and health professions students with a complete, clinically oriented understanding of neuroanatomy. Organized classically by system, this revised edition reflects the latest clinical approaches to neuroanatomy structures and reinforces concepts with enhanced, illustrations, diagnostic images, and surface anatomy photographs.

Each chapter begins with clear objectives and a clinical case for a practical introduction to key concepts. Throughout the text, Clinical Notes highlight important clinical considerations.Chapters end with bulleted key concepts, along with clinical problem solving cases and review questions that test students’ comprehension and ensure preparation for clinical application.},
  pagetotal = {560},
  date = {2018-11-06},
  author = {Splittgerber, Ryan}
}

@misc{adee2009cat,
  title = {Cat {{Fight Brews Over Cat Brain}}},
  url = {https://spectrum.ieee.org/tech-talk/semiconductors/devices/blue-brain-project-leader-angry-about-cat-brain},
  abstract = {Blue Brain’s Henry Markram says IBM lead researcher should be “strung up by the toes”},
  publisher = {{IEEE Spectrum}},
  urldate = {2020-01-04},
  date = {2009-11-23},
  author = {Adee, Sally}
}

@book{reichert2000neurobiologie,
  langid = {german},
  location = {{Stuttgart [u.a.]}},
  title = {Neurobiologie},
  edition = {2., neubearb. und erw. Aufl.},
  isbn = {3-13-745302-X},
  pagetotal = {VII, 250 S. : Ill., graph. Darst.},
  publisher = {{Thieme}},
  date = {2000},
  keywords = {Neurobiologie},
  author = {Reichert, Heinrich}
}

@article{abbott1999lapicque,
  title = {Lapicque’s Introduction of the Integrate-and-Fire Model Neuron (1907)},
  volume = {50},
  issn = {0361-9230},
  url = {http://www.sciencedirect.com/science/article/pii/S0361923099001616},
  doi = {https://doi.org/10.1016/S0361-9230(99)00161-6},
  number = {5},
  journaltitle = {Brain Research Bulletin},
  date = {1999},
  pages = {303-304},
  author = {Abbott, Laurence F.}
}

@article{lapicque1907recherches,
  langid = {french},
  title = {Recherches quantitatives sur l’excitation électrique des nerfs traitée comme une polarization},
  number = {9},
  journaltitle = {Journal de Physiologie et de Pathologie Generale},
  date = {1907},
  pages = {620-635},
  author = {Lapicque, Louis}
}

@article{hodgkin1952quantitative,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  volume = {117},
  number = {4},
  journaltitle = {The Journal of Physiology},
  date = {1952},
  pages = {500-544},
  author = {Hodgkin, Alan L. and Huxley, Andrew F.}
}


@book{traub1991neuronal,
  title = {Neuronal Networks of the Hippocampus},
  volume = {777},
  publisher = {{Cambridge University Press}},
  date = {1991},
  author = {Traub, Roger D. and Miles, Richard}
}

@book{rieke1999spikes,
  edition = {Reprinted ed.},
  title = {Spikes: {{Exploring}} the {{Neural Code}}},
  isbn = {978-0-262-68108-7},
  abstract = {Our perception of the world is driven by input from the sensory nerves. This input arrives encoded as sequences of identical spikes. Much of neural computation involves processing these spike trains. What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons.The authors invite the reader to play the role of a hypothetical observer inside the brain who makes decisions based on the incoming spike trains. Rather than asking how a neuron responds to a given stimulus, the authors ask how the brain could make inferences about an unknown stimulus from a given neural response. The flavor of some problems faced by the organism is captured by analyzing the way in which the observer can make a running reconstruction of the sensory stimulus as it evolves in time. These ideas are illustrated by examples from experiments on several biological systems.Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by "real" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory. A quantitative framework is used to pose precise questions about the structure of the neural code. These questions in turn influence both the design and analysis of experiments on sensory neurons.},
  author = {Rieke, Fred and Warland, David and Van Stevenick, Rob de Ruyter and Bialeck, William},
  month = jul,
  year = {1999},
}

@article{izhikevich2004model,
  title = {Which Model to Use for Cortical Spiking Neurons?},
  volume = {15},
  number = {5},
  journal = {IEEE transactions on neural networks},
  author = {Izhikevich, Eugene M.},
  year = {2004},
  pages = {1063-1070},
}

@article{izhikevich2003simple,
  title = {Simple Model of Spiking Neurons},
  volume = {14},
  number = {6},
  journal = {IEEE Transactions on neural networks},
  author = {Izhikevich, Eugene M.},
  year = {2003},
  pages = {1569-1572},
}

@article{hunsberger2015spiking,
  title = {Spiking {{Deep Networks}} with {{LIF Neurons}}},
  url = {http://arxiv.org/abs/1510.08829},
  abstract = {We train spiking deep networks using leaky integrate-and-fire (LIF) neurons, and achieve state-of-the-art results for spiking networks on the CIFAR-10 and MNIST datasets. This demonstrates that biologically-plausible spiking LIF neurons can be integrated into deep networks can perform as well as other spiking models (e.g. integrate-and-fire). We achieved this result by softening the LIF response function, such that its derivative remains bounded, and by training the network with noise to provide robustness against the variability introduced by spikes. Our method is general and could be applied to other neuron types, including those used on modern neuromorphic hardware. Our work brings more biological realism into modern image classification models, with the hope that these models can inform how the brain performs this difficult task. It also provides new methods for training deep networks to run on neuromorphic hardware, with the aim of fast, power-efficient image classification for robotics applications.},
  journaltitle = {arXiv:1510.08829},
  date = {2015},
  author = {Hunsberger, Eric and Eliasmith, Chris}
}

@article{hubel1959receptive,
  title = {Receptive {{Fields}} of {{Single Neurones}} in the {{Cat}}'s {{Striate Cortex}}},
  volume = {148},
  url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1959.sp006308},
  doi = {10.1113/jphysiol.1959.sp006308},
  number = {3},
  journaltitle = {The Journal of physiology},
  date = {1959},
  pages = {574-591},
  author = {Hubel, David H. and Wiesel, Torsten N.}
}


@misc{2009itur,
  title = {{{ITU}}-{{R}}  {{M}}.1677-1: {{International Morse Code}}},
  date = {2009-10},
  publisher = {{International Telecommunication Union}},
  url = {https://www.itu.int/rec/R-REC-M.1677-1-200910-I/}
}

@misc{mann1997nervous,
  title = {The {{Nervous System In Action}}},
  author = {Mann, Michael D.},
  year = {1997},
  url = {http://michaeldmann.net/The\%20Nervous\%20System\%20In\%20Action.html},
  urldate = {2020-01-13}
}

@incollection{katsuki1969neural,
  title = {Neural Mechanism of Auditory Sensation in Cats},
  booktitle = {Sensory Communication},
  author = {Katsuki, Y.},
  editor = {Rosenblith, W.A.},
  year = {1969},
  publisher = {{MIT Press}},
  isbn = {978-0-262-51842-0},
  lccn = {2013370006}
}

@article{saleem2013integration,
  title = {Integration of Visual Motion and Locomotion in Mouse Visual Cortex},
  author = {Saleem, Aman B and Ayaz, Asl\i{} and Jeffery, Kathryn J and Harris, Kenneth D and Carandini, Matteo},
  year = {2013},
  month = dec,
  volume = {16},
  pages = {1864--1869},
  issn = {1546-1726},
  doi = {10.1038/nn.3567},
  abstract = {The primary visual cortex (V1) carries signals related to visual speed, and its responses are also affected by run speed. Here the authors report that nearly half of the V1 neurons were reliably driven by combinations of visual speed and run speed. As a population, V1 neurons predicted a linear combination of visual and run speed better than visual or run speeds alone.},
  journal = {Nature Neuroscience},
  number = {12}
}

@article{gabor1946theory,
  title = {Theory of Communication. {{Part}} 1: {{The}} Analysis of Information},
  author = {Gabor, Dennis},
  date = {1946},
  journaltitle = {Journal of the Institution of Electrical Engineers-Part III: Radio and Communication Engineering},
  volume = {93},
  pages = {429--441},
  publisher = {{IET}},
  number = {26}
}

@book{gerstner2002spiking,
  title = {Spiking {{Neuron Models}}: {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  date = {2002},
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-89079-3},
  note = {LCCB: 2002067657}
}

@article{sparks2002brainstem,
  title = {The Brainstem Control of Saccadic Eye Movements},
  author = {Sparks, David L.},
  date = {2002-12-01},
  journaltitle = {Nature Reviews Neuroscience},
  shortjournal = {Nature Reviews Neuroscience},
  volume = {3},
  pages = {952--964},
  issn = {1471-0048},
  doi = {10.1038/nrn986},
  url = {https://doi.org/10.1038/nrn986},
  abstract = {The oculomotor system is a useful model for the study of purposeful movements. Rotations of the eyes are produced by three pairs of extraocular muscles that are innervated by motor neurons from the III, IV and VI cranial nerve nuclei. A saccadic eye movement is produced when the appropriate motor neurons produce a burst of spikes, followed by tonic firing at the correct rate to maintain the eye's new position. The commands for horizontal components of saccades originate in the pons and medulla. Omnipause neurons fire tonically during fixation, but stop firing during saccades; long-lead burst neurons and excitatory burst neurons fire before and during saccades. Excitatory burst neurons synapse onto motor neurons and drive the motor neuron pulse of activity. Neurons in the prepositus and vestibular nuclei fire tonically and drive the step of activity that maintains eye position. Microstimulation of neurons in the pontine reticular formation produces horizontal eye movements. Neurons in the rostral midbrain that have similar properties drive vertical eye movements. The duration and velocity of saccades are determined by the duration and maximal firing rate of the bursts of activity produced by these neurons. The horizontal and vertical components of oblique saccades are coordinated. The shorter of the two components proceeds at a lower velocity than normal so that the two will have the same duration and the movement will not be curved. The onsets of the two components are coordinated by omnipause neurons in the pons. The torsional component of saccadic eye movements is stereotyped and obeys Donders' law: for any direction of the line of sight, if the head is upright and stationary, the eye will assume a given degree of torsion, regardless of the route taken by the eye to reach its position. Saccades also obey Listing's law, which specifies the orientation of the globe for each gaze position. It is unclear how these laws are implemented, but identifying the mechanisms is an important part of understanding the generation of saccades. Neurons in the superior colliculus (SC) provide the main input to the pontine and midbrain pulse–step generators. Microstimulation of these neurons produces saccades in head-restrained animals, and coordinated head and eye movements in head-unrestrained animals. The size and direction of the movements produced are primarily determined by the position of the stimulation in the SC. Models of saccade generation assume that saccades are under feedback control and that the feedback comes from corollary discharge. Models differ in the implementation of this feedback, but it has not been possible to determine the type of feedback signal used or the site of the comparator. Reasons for this include the fact that signals that are separate in models might be intermingled in the brain, preventing selective lesioning of these signals, and that different models produce similar signals. Models also tend to use population signals, but in terms of electrophysiology, it is hard to know what the population signal is at any given time. The problems faced by oculomotor researchers are mirrored in other areas of systems neuroscience.},
  number = {12}
}

@article{georgopoulos1982relations,
  title = {On the Relations between the Direction of Two-Dimensional Arm Movements and Cell Discharge in Primate Motor Cortex},
  author = {Georgopoulos, A P and Kalaska, J F and Caminiti, R and Massey, J T},
  date = {1982-11},
  journaltitle = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
  shortjournal = {J Neurosci},
  volume = {2},
  pages = {1527--1537},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.02-11-01527.1982},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/7143039},
  abstract = {The activity of single cells in the motor cortex was recorded while monkeys made arm movements in eight directions (at 45 degrees intervals) in a two-dimensional apparatus. These movements started from the same point and were of the same amplitude. The activity of 606 cells related to proximal arm movements was examined in the task; 323 of the 606 cells were active in that task and were studied in detail. The frequency of discharge of 241 of the 323 cells (74.6\%) varied in an orderly fashion with the direction of movement. Discharge was most intense with movements in a preferred direction and was reduced gradually when movements were made in directions farther and farther away from the preferred one. This resulted in a bell-shaped directional tuning curve. These relations were observed for cell discharge during the reaction time, the movement time, and the period that preceded the earliest changes in the electromyographic activity (approximately 80 msec before movement onset). In about 75\% of the 241 directionally tuned cells, the frequency of discharge, D, was a sinusoidal function of the direction of movement, theta: D = b0 + b1 sin theta + b2cos theta, or, in terms of the preferred direction, theta 0: D = b0 + c1cos (theta - theta0), where b0, b1, b2, and c1 are regression coefficients. Preferred directions differed for different cells so that the tuning curves partially overlapped. The orderly variation of cell discharge with the direction of movement and the fact that cells related to only one of the eight directions of movement tested were rarely observed indicate that movements in a particular direction are not subserved by motor cortical cells uniquely related to that movement. It is suggested, instead, that a movement trajectory in a desired direction might be generated by the cooperation of cells with overlapping tuning curves. The nature of this hypothetical population code for movement direction remains to be elucidated.},
  eprint = {7143039},
  eprinttype = {pubmed},
  keywords = {*Movement,Animals,Arm/physiology,Biomechanical Phenomena,Electromyography,Macaca mulatta,Male,Motor Cortex/*physiology,Neurons/physiology},
  langid = {english},
  number = {11}
}

@article{burke1967composite,
  title = {Composite Nature of the Monosynaptic Excitatory Postsynaptic Potential.},
  author = {Burke, R E},
  date = {1967},
  journaltitle = {Journal of Neurophysiology},
  volume = {30},
  pages = {1114--1137},
  doi = {10.1152/jn.1967.30.5.1114},
  url = {https://doi.org/10.1152/jn.1967.30.5.1114},
  eprint = {https://doi.org/10.1152/jn.1967.30.5.1114},
  number = {5}
}

@article{voelker2018improving,
  title = {Improving {{Spiking Dynamical Networks}}: {{Accurate Delays}}, {{Higher}}-{{Order Synapses}}, and {{Time Cells}}},
  author = {Voelker, Aaron R. and Eliasmith, Chris},
  date = {2018-03},
  journaltitle = {Neural Computation},
  volume = {30},
  pages = {569--609},
  publisher = {MIT Press},
  doi = {10.1162/neco_a_01046},
  url = {https://www.mitpressjournals.org/doi/abs/10.1162/neco_a_01046},
  abstract = {Researchers building spiking neural networks face the challenge of improving the biological plausibility of their model networks while maintaining the ability to quantitatively characterize network behavior. In this work, we extend the theory behind the neural engineering framework (NEF), a method of building spiking dynamical networks, to permit the use of a broad class of synapse models while maintaining prescribed dynamics up to a given order. This theory improves our understanding of how low-level synaptic properties alter the accuracy of high-level computations in spiking dynamical networks. For completeness, we provide characterizations for both continuous-time (i.e., analog) and discrete-time (i.e., digital) simulations. We demonstrate the utility of these extensions by mapping an optimal delay line onto various spiking dynamical networks using higher-order models of the synapse. We show that these networks nonlinearly encode rolling windows of input history, using a scale invariant representation, with accuracy depending on the frequency content of the input signal. Finally, we reveal that these methods provide a novel explanation of time cell responses during a delay task, which have been observed throughout hippocampus, striatum, and cortex.},
  number = {3}
}


@inproceedings{voelker2019lmu,
  title = {Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Voelker, Aaron R. and Kaji{\'{c}}, Ivana and Eliasmith, Chris},
  date = {2019},
  abstract = {We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit (LMU) is mathematically derived to orthogonalize its continuous-time history – doing so by solving d coupled ordinary differential equations (ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree d - 1. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning 100000 time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time – exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network's disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using m recurrently-connected Poisson spiking neurons, ( m ) time and memory, with error scaling as ( d /  ). We discuss implementations of LMUs on analog and digital neuromorphic hardware.},
}

@thesis{voelker2019thesis,
  title = {Dynamical Systems in Spiking Neuromorphic Hardware},
  author = {Voelker, Aaron R.},
  date = {2019},
  institution = {{University of Waterloo}},
  location = {{Waterloo, ON}},
  url = {http://hdl.handle.net/10012/14625},
  abstract = {Dynamical systems are universal computers. They can perceive stimuli, remember, learn from feedback, plan sequences of actions, and coordinate complex behavioural responses. The Neural Engineering Framework (NEF) provides a general recipe to formulate models of such systems as coupled sets of nonlinear differential equations and compile them onto recurrently connected spiking neural networks – akin to a programming language for spiking models of computation. The Nengo software ecosystem supports the NEF and compiles such models onto neuromorphic hardware. In this thesis, we analyze the theory driving the success of the NEF, and expose several core principles underpinning its correctness, scalability, completeness, robustness, and extensibility. We also derive novel theoretical extensions to the framework that enable it to far more effectively leverage a wide variety of dynamics in digital hardware, and to exploit the device-level physics in analog hardware. At the same time, we propose a novel set of spiking algorithms that recruit an optimal nonlinear encoding of time, which we call the Delay Network (DN). Backpropagation across stacked layers of DNs dramatically outperforms stacked Long Short-Term Memory (LSTM) networks—a state-of-the-art deep recurrent architecture—in accuracy and training time, on a continuous-time memory task, and a chaotic time-series prediction benchmark. The basic component of this network is shown to function on state-of-the-art spiking neuromorphic hardware including Braindrop and Loihi. This implementation approaches the energy-efficiency of the human brain in the former case, and the precision of conventional computation in the latter case.},
  pdf = {https://uwspace.uwaterloo.ca/bitstream/handle/10012/14625/Voelker\textsubscript{A}aron.pdf},
  type = {PhD thesis}
}
