@misc{richard1988richard,
  title = {Richard {{Feynman}}'s Blackboard at Time of His Death},
  url = {http://archives-dc.library.caltech.edu/islandora/object/ct1:483},
  publisher = {{The Caltech Archives}},
  urldate = {2019-12-30},
  date = {1988},
  author = {Feynman, Richard}
}

@book{eliasmith2013how,
  location = {{New York, New York}},
  title = {How to Build a Brain: {{A}} Neural Architecture for Biological Cognition},
  isbn = {978-0-19-026212-9},
  pagetotal = {456},
  series = {Oxford {{Series}} on {{Cognitive Models}} and {{Architectures}}},
  publisher = {{Oxford University Press}},
  date = {2013},
  author = {Eliasmith, Chris},
  organization = {Oxford University Press}
}

@book{eliasmith2003neural,
  location = {{Cambridge, Massachusetts}},
  title = {Neural Engineering: {{Computation}}, Representation, and Dynamics in Neurobiological Systems},
  isbn = {978-0-262-55060-4},
  pagetotal = {380},
  publisher = {{MIT Press}},
  date = {2003},
  author = {Eliasmith, Chris and Anderson, Charles H.}
}

@article{bekolay2014nengo,
  title = {Nengo: {{A Python}} Tool for Building Large-Scale Functional Brain Models},
  volume = {7},
  abstract = {Neuroscience currently lacks a comprehensive theory of how cognitive processes can be implemented in a biological substrate. The Neural Engineering Framework (NEF) proposes one such theory, but has not yet gathered significant empirical support, partly due to the technical challenge of building and simulating large-scale models with the NEF. Nengo is a software tool that can be used to build and simulate large-scale models based on the NEF; currently, it is the primary resource for both teaching how the NEF is used, and for doing research that generates specific NEF models to explain experimental data. Nengo 1.4, which was implemented in Java, was used to create Spaun, the world's largest functional brain model (Eliasmith et al., 2012). Simulating Spaun highlighted limitations in Nengo 1.4's ability to support model construction with simple syntax, to simulate large models quickly, and to collect large amounts of data for subsequent analysis. This paper describes Nengo 2.0, which is implemented in Python and overcomes these limitations. It uses simple and extendable syntax, simulates a benchmark model on the scale of Spaun 50 times faster than Nengo 1.4, and has a flexible mechanism for collecting simulation results.},
  number = {48},
  journaltitle = {Frontiers in Neuroinformatics},
  date = {2014},
  author = {Bekolay, Trevor and Bergstra, James and Hunsberger, Eric and DeWolf, Travis and Stewart, Terrence C and Rasmussen, Daniel and Choo, Xuan and Voelker, Aaron R. and Eliasmith, Chris}
}

@article{hafting2005microstructure,
  title = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  volume = {436},
  issn = {1476-4687},
  url = {https://doi.org/10.1038/nature03721},
  doi = {10.1038/nature03721},
  abstract = {The ability to find one's way depends on neural algorithms that integrate information about place, distance and direction, but the implementation of these operations in cortical microcircuits is poorly understood. Here we show that the dorsocaudal medial entorhinal cortex (dMEC) contains a directionally oriented, topographically organized neural map of the spatial environment. Its key unit is the ‘grid cell’, which is activated whenever the animal's position coincides with any vertex of a regular grid of equilateral triangles spanning the surface of the environment. Grids of neighbouring cells share a common orientation and spacing, but their vertex locations (their phases) differ. The spacing and size of individual fields increase from dorsal to ventral dMEC. The map is anchored to external landmarks, but persists in their absence, suggesting that grid cells may be part of a generalized, path-integration-based map of the spatial environment.},
  number = {7052},
  journaltitle = {Nature},
  shortjournal = {Nature},
  date = {2005-08-01},
  pages = {801-806},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.},
}

@book{okeefe1978hippocampus,
  location = {{Oxford, United Kingdom}},
  title = {The {{Hippocampus}} as a {{Cognitive Map}}},
  isbn = {0-19-857206-9},
  url = {http://cognitivemap.net/},
  publisher = {{Oxford University Press}},
  date = {1978},
  author = {O'Keefe, John and Nadel, Lynn},
}

@article{sussillo2009generating,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  volume = {63},
  issn = {0896-6273},
  url = {http://www.sciencedirect.com/science/article/pii/S0896627309005479},
  doi = {https://doi.org/10.1016/j.neuron.2009.07.018},
  abstract = {Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  number = {4},
  journaltitle = {Neuron},
  date = {2009},
  pages = {544-557},
  keywords = {SYSNEURO},
  author = {Sussillo, David and Abbott, L.F.},
}


@article{nicola2017supervised,
  title = {Supervised Learning in Spiking Neural Networks with {{FORCE}} Training},
  volume = {8},
  issn = {2041-1723},
  url = {https://doi.org/10.1038/s41467-017-01827-3},
  doi = {10.1038/s41467-017-01827-3},
  abstract = {Populations of neurons display an extraordinary diversity in the behaviors they affect and display. Machine learning techniques have recently emerged that allow us to create networks of model neurons that display behaviors of similar complexity. Here we demonstrate the direct applicability of one such technique, the FORCE method, to spiking neural networks. We train these networks to mimic dynamical systems, classify inputs, and store discrete sequences that correspond to the notes of a song. Finally, we use FORCE training to create two biologically motivated model circuits. One is inspired by the zebra finch and successfully reproduces songbird singing. The second network is motivated by the hippocampus and is trained to store and replay a movie scene. FORCE trained networks reproduce behaviors comparable in complexity to their inspired circuits and yield information not easily obtainable with other techniques, such as behavioral responses to pharmacological manipulations and spike timing statistics.},
  number = {1},
  journaltitle = {Nature Communications},
  shortjournal = {Nature Communications},
  date = {2017-12-20},
  pages = {2208},
  author = {Nicola, Wilten and Clopath, Claudia},
}

@article{boerlin2011spikebased,
  title = {Spike-{{Based Population Coding}} and {{Working Memory}}},
  volume = {7},
  url = {https://doi.org/10.1371/journal.pcbi.1001080},
  doi = {10.1371/journal.pcbi.1001080},
  abstract = {Author Summary Most of our daily actions are subject to uncertainty. Behavioral studies have confirmed that humans handle this uncertainty in a statistically optimal manner. A key question then is what neural mechanisms underlie this optimality, i.e. how can neurons represent and compute with probability distributions. Previous approaches have proposed that probabilities are encoded in the firing rates of neural populations. However, such rate codes appear poorly suited to understand perception in a constantly changing environment. In particular, it is unclear how probabilistic computations could be implemented by biologically plausible spiking neurons. Here, we propose a network of spiking neurons that can optimally combine uncertain information from different sensory modalities and keep this information available for a long time. This implies that neural memories not only represent the most likely value of a stimulus but rather a whole probability distribution over it. Furthermore, our model suggests that each spike conveys new, essential information. Consequently, the observed variability of neural responses cannot simply be understood as noise but rather as a necessary consequence of optimal sensory integration. Our results therefore question strongly held beliefs about the nature of neural “signal” and “noise”.},
  number = {2},
  journaltitle = {PLOS Computational Biology},
  date = {2011-02},
  pages = {1-18},
  author = {Boerlin, Martin and Denève, Sophie},
  publisher = {Public Library of Science}
}

@article{boerlin2013predictive,
  title = {Predictive {{Coding}} of {{Dynamical Variables}} in {{Balanced Spiking Networks}}},
  volume = {9},
  url = {https://doi.org/10.1371/journal.pcbi.1003258},
  doi = {10.1371/journal.pcbi.1003258},
  abstract = {Author Summary Two observations about the cortex have puzzled and fascinated neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks representing information reliably and with a small number of spikes. To achieve such efficiency, spikes of individual neurons must communicate prediction errors about a common population-level signal, automatically resulting in balanced excitation and inhibition and highly variable neural responses. We illustrate our approach by focusing on the implementation of linear dynamical systems. Among other things, this allows us to construct a network of spiking neurons that can integrate input signals, yet is robust against many perturbations. Most importantly, our approach shows that neural variability cannot be equated to noise. Despite exhibiting the same single unit properties as other widely used network models, our balanced networks are orders of magnitudes more reliable. Our results suggest that the precision of cortical representations has been strongly underestimated.},
  number = {11},
  journaltitle = {PLOS Computational Biology},
  date = {2013-11},
  pages = {1-16},
  author = {Boerlin, Martin and Machens, Christian K. and Denève, Sophie},
  publisher = {Public Library of Science}
}


@article{eliasmith2005unified,
  title = {A Unified Approach to Building and Controlling Spiking Attractor Networks},
  volume = {7},
  number = {6},
  journaltitle = {Neural computation},
  date = {2005},
  pages = {1276-1314},
  author = {Eliasmith, Chris},
  type = {article}
}

@book{abbott2001theoretical,
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  isbn = {978-0-262-04199-7},
  url = {https://mitpress.mit.edu/books/theoretical-neuroscience},
  abstract = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.

The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  pagetotal = {480},
  series = {Computational {{Neuroscience}}},
  publisher = {{MIT Press}},
  date = {2001-10},
  author = {Abbott, Laurence F. and Dayan, Peter},
}

@mastersthesis{stoeckel2015design,
  location = {{Germany}},
  title = {Design Space Exploration of Associative Memories Using Spiking Neurons with Respect to Neuromorphic Hardware Implementations},
  institution = {{Bielefeld University}},
  date = {2015},
  author = {Stöckel, Andreas}
}

@book{kandel2012principles,
  title = {Principles of {{Neural Science}}},
  edition = {5},
  publisher = {{McGraw-Hill Education}},
  date = {2012},
  author = {Kandel, E. and Schwartz, J. and Jessell, T. and Siegelbaum, S. and Hudspeth, A. J.}
}

@article{chedotal2010wiring,
  langid = {english},
  title = {Wiring the Brain: The Biology of Neuronal Guidance},
  volume = {2},
  issn = {1943-0264},
  url = {https://www.ncbi.nlm.nih.gov/pubmed/20463002},
  doi = {10.1101/cshperspect.a001917},
  abstract = {The mammalian brain is the most complex organ in the body. It controls all aspects of our bodily functions and interprets the world around us through our senses. It defines us as human beings through our memories and our ability to plan for the future. Crucial to all these functions is how the brain is wired in order to perform these tasks. The basic map of brain wiring occurs during embryonic and postnatal development through a series of precisely orchestrated developmental events regulated by specific molecular mechanisms. Below we review the most important features of mammalian brain wiring derived from work in both mammals and in nonmammalian species. These mechanisms are highly conserved throughout evolution, simply becoming more complex in the mammalian brain. This fascinating area of biology is uncovering the essence of what makes the mammalian brain able to perform the everyday tasks we take for granted, as well as those which give us the ability for extraordinary achievement.},
  number = {6},
  journaltitle = {Cold Spring Harbor perspectives in biology},
  shortjournal = {Cold Spring Harb Perspect Biol},
  date = {2010-06},
  pages = {a001917-a001917},
  keywords = {Humans,Animals,Axons/physiology,Brain/*cytology/*physiology,Gene Expression Regulation/physiology,Mammals/*physiology,Neurons/*cytology/*physiology},
  author = {Chédotal, Alain and Richards, Linda J},
  eprinttype = {pubmed},
  eprint = {20463002}
}

@article{kanwisher1997fusiform,
  title = {The Fusiform Face Area: {{A}} Module in Human Extrastriate Cortex Specialized for Face Perception},
  volume = {17},
  issn = {0270-6474},
  url = {https://www.jneurosci.org/content/17/11/4302},
  doi = {10.1523/JNEUROSCI.17-11-04302.1997},
  abstract = {Using functional magnetic resonance imaging (fMRI), we found an area in the fusiform gyrus in 12 of the 15 subjects tested that was significantly more active when the subjects viewed faces than when they viewed assorted common objects. This face activation was used to define a specific region of interest individually for each subject, within which several new tests of face specificity were run. In each of five subjects tested, the predefined candidate face area also responded significantly more strongly to passive viewing of (1) intact than scrambled two-tone faces, (2) full front-view face photos than front-view photos of houses, and (in a different set of five subjects) (3) three-quarter-view face photos (with hair concealed) than photos of human hands; it also responded more strongly during (4) a consecutive matching task performed on three-quarter-view faces versus hands. Our technique of running multiple tests applied to the same region defined functionally within individual subjects provides a solution to two common problems in functional imaging: (1) the requirement to correct for multiple statistical comparisons and (2) the inevitable ambiguity in the interpretation of any study in which only two or three conditions are compared. Our data allow us to reject alternative accounts of the function of the fusiform face area (area FF) that appeal to visual attention, subordinate-level classification, or general processing of any animate or human forms, demonstrating that this region is selectively involved in the perception of faces.},
  number = {11},
  journaltitle = {Journal of Neuroscience},
  date = {1997},
  pages = {4302-4311},
  author = {Kanwisher, Nancy and McDermott, Josh and Chun, Marvin M.},
  eprint = {https://www.jneurosci.org/content/17/11/4302.full.pdf},
  publisher = {{Society for Neuroscience}}
}

@article{markram2012human,
  title = {The {{Human Brain Project}}},
  volume = {306},
  number = {6},
  journaltitle = {Scientific American},
  date = {2012},
  pages = {50-55},
  author = {Markram, Henry}
}

@article{merolla2014million,
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  volume = {345},
  issn = {0036-8075},
  url = {https://science.sciencemag.org/content/345/6197/668},
  doi = {10.1126/science.1254642},
  abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brains structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  number = {6197},
  journaltitle = {Science},
  date = {2014},
  pages = {668-673},
  author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  eprint = {https://science.sciencemag.org/content/345/6197/668.full.pdf},
  publisher = {{American Association for the Advancement of Science}}
}

@article{morgan2011darpa,
  entrysubtype = {newspaper},
  title = {{{DARPA}} Shells out \$21m for {{IBM}} Cat Brain Chip},
  url = {https://www.theregister.co.uk/2011/08/18/ibm_darpa_synapse_project/},
  journaltitle = {The Register},
  journalsubtitle = {Science},
  urldate = {2020-01-03},
  date = {2011-08-18},
  author = {Morgan, Timothy P.}
}

@article{komer2016unified,
  title = {A Unified Theoretical Approach for Biological Cognition and Learning},
  volume = {11},
  issn = {2352-1546},
  url = {http://www.sciencedirect.com/science/article/pii/S2352154616300651},
  doi = {https://doi.org/10.1016/j.cobeha.2016.03.006},
  abstract = {Large-scale neural models are needed in order to understand the biological underpinnings of complex cognitive behavior. Good methods for constructing such models should provide for: first, abstraction (analysis across levels of description); second, integration (incorporation of simpler models to build more complex ones); third, empirical contact (using and comparing to a wide variety of neural data); and fourth, account for the varieties of learning. In this review we evaluate three prominent recent methods for constructing neural models using these four criteria. Each of these methods is being actively developed and demonstrates clear strengths along some of these criteria.},
  journaltitle = {Current Opinion in Behavioral Sciences},
  date = {2016},
  pages = {14-20},
  author = {Komer, Brent and Eliasmith, Chris},
  note = {Computational modeling}
}

@article{jonas2017could,
  title = {Could a Neuroscientist Understand a Microprocessor?},
  volume = {13},
  url = {https://doi.org/10.1371/journal.pcbi.1005268},
  doi = {10.1371/journal.pcbi.1005268},
  abstract = {Author Summary Neuroscience is held back by the fact that it is hard to evaluate if a conclusion is correct; the complexity of the systems under study and their experimental inaccessability make the assessment of algorithmic and data analytic technqiues challenging at best. We thus argue for testing approaches using known artifacts, where the correct interpretation is known. Here we present a microprocessor platform as one such test case. We find that many approaches in neuroscience, when used naïvely, fall short of producing a meaningful understanding.},
  number = {1},
  journaltitle = {PLOS Computational Biology},
  date = {2017-01},
  pages = {1-24},
  author = {Jonas, Eric and Kording, Konrad Paul},
  publisher = {{Public Library of Science}}
}

@article{boahen2017neuromorph,
  title = {A Neuromorph's Prospectus},
  volume = {19},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2017.33},
  number = {2},
  journaltitle = {Computing in Science Engineering},
  date = {2017-03},
  pages = {14-28},
  keywords = {neuromorphic,computational neuroscience,neural engineering,Neuromorphics,Neural networks,Neuroscience,all-analog computing,all-digital computing,analog dendritic computation,analogue integrated circuits,asynchronous circuits,asynchronous digital circuits,asynchronous logic,autonomous robots,axons,cognitive computing,continuous signal encoding,dendrite emulation,digital axonal communication,Digital communication,digital computers,digital integrated circuits,electron traffic,embedded computing,Energy efficiency,error tolerance,ion channels,mapping arbitrary computations,mixed analog-digital systems,Moore's Law,Nanoscale devices,nanoscale dimensions,network-on-a-chip,neural networks,neurogrid neuromorphic system,neuromorph prospectus,neuromorphic chips,neuromorphic engineers,scientific computing,shrink transistors,single-lane nanoscale devices,spike trains,subthreshold analog circuits,subthreshold circuits,synaptic connections,system-on-a-chip,Three-dimensional displays,Transistors,trapped electrons--blocking lanes,Very large scale integration,VLSI},
  author = {Boahen, Kwabena}
}

@book{splittgerber2018snell,
  title = {Snell's {{Clinical Neuroanatomy}}},
  edition = {8th Edition},
  abstract = {Snell’s Clinical Neuroanatomy , Eighth Edition, equips medical and health professions students with a complete, clinically oriented understanding of neuroanatomy. Organized classically by system, this revised edition reflects the latest clinical approaches to neuroanatomy structures and reinforces concepts with enhanced, illustrations, diagnostic images, and surface anatomy photographs.

Each chapter begins with clear objectives and a clinical case for a practical introduction to key concepts. Throughout the text, Clinical Notes highlight important clinical considerations.Chapters end with bulleted key concepts, along with clinical problem solving cases and review questions that test students’ comprehension and ensure preparation for clinical application.},
  pagetotal = {560},
  date = {2018-11-06},
  author = {Splittgerber, Ryan}
}

@misc{adee2009cat,
  title = {Cat {{Fight Brews Over Cat Brain}}},
  url = {https://spectrum.ieee.org/tech-talk/semiconductors/devices/blue-brain-project-leader-angry-about-cat-brain},
  abstract = {Blue Brain’s Henry Markram says IBM lead researcher should be “strung up by the toes”},
  publisher = {{IEEE Spectrum}},
  urldate = {2020-01-04},
  date = {2009-11-23},
  author = {Adee, Sally}
}

@book{reichert2000neurobiologie,
  langid = {german},
  location = {{Stuttgart [u.a.]}},
  title = {Neurobiologie},
  edition = {2., neubearb. und erw. Aufl.},
  isbn = {3-13-745302-X},
  pagetotal = {VII, 250 S. : Ill., graph. Darst.},
  publisher = {{Thieme}},
  date = {2000},
  keywords = {Neurobiologie},
  author = {Reichert, Heinrich}
}

@article{abbott1999lapicque,
  title = {Lapicque’s Introduction of the Integrate-and-Fire Model Neuron (1907)},
  volume = {50},
  issn = {0361-9230},
  url = {http://www.sciencedirect.com/science/article/pii/S0361923099001616},
  doi = {https://doi.org/10.1016/S0361-9230(99)00161-6},
  number = {5},
  journaltitle = {Brain Research Bulletin},
  date = {1999},
  pages = {303-304},
  author = {Abbott, L.F}
}

@article{lapicque1907recherches,
  langid = {french},
  title = {Recherches quantitatives sur l’excitation électrique des nerfs traitée comme une polarization},
  number = {9},
  journaltitle = {Journal de Physiologie et de Pathologie Generale},
  date = {1907},
  pages = {620-635},
  author = {Lapicque, Louis}
}

@article{hodgkin1952quantitative,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  volume = {117},
  number = {4},
  journaltitle = {The Journal of Physiology},
  date = {1952},
  pages = {500-544},
  author = {Hodgkin, Alan L. and Huxley, Andrew F.}
}


@book{traub1991neuronal,
  title = {Neuronal Networks of the Hippocampus},
  volume = {777},
  publisher = {{Cambridge University Press}},
  date = {1991},
  author = {Traub, Roger D. and Miles, Richard}
}

@book{rieke1999spikes,
  edition = {Reprinted ed.},
  title = {Spikes: {{Exploring}} the {{Neural Code}}},
  isbn = {978-0-262-68108-7},
  abstract = {Our perception of the world is driven by input from the sensory nerves. This input arrives encoded as sequences of identical spikes. Much of neural computation involves processing these spike trains. What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons.The authors invite the reader to play the role of a hypothetical observer inside the brain who makes decisions based on the incoming spike trains. Rather than asking how a neuron responds to a given stimulus, the authors ask how the brain could make inferences about an unknown stimulus from a given neural response. The flavor of some problems faced by the organism is captured by analyzing the way in which the observer can make a running reconstruction of the sensory stimulus as it evolves in time. These ideas are illustrated by examples from experiments on several biological systems.Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by "real" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory. A quantitative framework is used to pose precise questions about the structure of the neural code. These questions in turn influence both the design and analysis of experiments on sensory neurons.},
  author = {Rieke, Fred and Warland, David and Van Stevenick, Rob de Ruyter and Bialeck, William},
  month = jul,
  year = {1999},
}

@article{izhikevich2004model,
  title = {Which Model to Use for Cortical Spiking Neurons?},
  volume = {15},
  number = {5},
  journal = {IEEE transactions on neural networks},
  author = {Izhikevich, Eugene M},
  year = {2004},
  pages = {1063-1070},
}

@article{izhikevich2003simple,
  title = {Simple Model of Spiking Neurons},
  volume = {14},
  number = {6},
  journal = {IEEE Transactions on neural networks},
  author = {Izhikevich, Eugene M.},
  year = {2003},
  pages = {1569-1572},
}
